{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6495c181-7ccb-422b-842c-c19d7d7f5581",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model and Spectral Clustering\n",
    "\n",
    "In the following notebook, we take a look at the efficacy of GMMs and spectral clustering on our dimensionality reduced data. Our main scoring method here will be the Adjusted Rand Index (ARI). If any clustering's ARI scores high (close to 1), then that indicates a good clustering that matches the galaxy labels well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbe3779-46f7-4118-8a1c-7f09fd388a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150fe68-9f08-453c-8a7e-65f58f72ec91",
   "metadata": {},
   "source": [
    "The following cell opens the normalized images and does PCA using `n_components = 35`, a number which was determined in `Data_Cleaning.ipynb`. This takes several minutes and since we are only interested in using the dimensionality reduced data, this cell should only be run once to write data to a new file, enabling faster loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c0fbbc-969c-474a-9e3b-d840a86ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattened_image_dir= \"processed_galaxy_data/flattened_normalized_images.csv\"\n",
    "\n",
    "# # Load flattened images\n",
    "# flattened_data = pd.read_csv(flattened_image_dir, header = None)\n",
    "\n",
    "# # Declaring and fitting our PCA\n",
    "# pca = PCA(n_components = 35)\n",
    "# fitted_pca = pca.fit_transform(flattened_data)\n",
    "\n",
    "# # Writing to file.\n",
    "# np.savetxt('processed_galaxy_data/pca_normalized.csv', fitted_pca, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e626a9a0-bbb3-44a3-abe0-f4535cf1b313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitted_pca = pd.read_csv(\"processed_galaxy_data/pca_normalized.csv\", header = None)\n",
    "labels = pd.read_csv(\"processed_galaxy_data/labels_mappings_filtered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfe3b3-4ce3-4bc7-9735-3041786d3386",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model\n",
    "\n",
    "We want to look into how GMM will cluster our data and its effectiveness.\\\n",
    "First, we get a random subset of the data. This hopefully ensures that a biased sample is not chosen.\\\n",
    "Next, we declare the GMM and its parameters. Since we know that our data has 4 labels, we can explicitly set `n_components = 4`. GMMs have a parameter to dictate the covariance type to optimize over. We set up a grid search for the multiple types of covariance and cross validate over our randomly chosen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0c248a-6c65-466c-99a1-31823a2d0649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shape_label\n",
       "E     97643\n",
       "S     95818\n",
       "SB    45568\n",
       "A       544\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fitted_pca, labels[\"shape_label\"], train_size=0.8, random_state=42)\n",
    "labels[\"shape_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0a830ec-4335-4850-97f3-a9adf4da4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components = 4)\n",
    "pipe_gmm = Pipeline([\n",
    "    (\"gmm\", gmm)\n",
    "])\n",
    "param_grid_gmm = {\n",
    "    \"gmm__covariance_type\": [\"full\", \"tied\", \"spherical\", \"diag\"]\n",
    "}\n",
    "grid_gmm = GridSearchCV(pipe_gmm, param_grid_gmm, scoring = \"adjusted_rand_score\", n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47ede9c-b899-42a7-b1ba-72ada945b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_gmm = grid_gmm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d08c88-87ee-4199-9b7f-8c002e795509",
   "metadata": {},
   "source": [
    "We used the Adjusted Rand Index to score our models. We can display a DataFrame, which shows us the covariance type and the associated Mean ARI calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f3316e-e18f-4b00-ba77-3b99e853a796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gmm__covariance_type</th>\n",
       "      <th>Mean Adjusted Rand Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>full</td>\n",
       "      <td>0.000124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tied</td>\n",
       "      <td>0.000262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spherical</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diag</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gmm__covariance_type  Mean Adjusted Rand Index\n",
       "0                 full                  0.000124\n",
       "1                 tied                  0.000262\n",
       "2            spherical                  0.000049\n",
       "3                 diag                  0.000070"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_gmm = pd.DataFrame(grid_gmm.cv_results_['params'] )\n",
    "results_gmm['Mean Adjusted Rand Index'] = grid_gmm.cv_results_['mean_test_score']\n",
    "results_gmm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c79a5-58ed-4f72-ba1f-cd50b78137d4",
   "metadata": {},
   "source": [
    "Our scores are fairly poor -- almost close to 0. Let's try another clustering method to see if results are any better.\n",
    "\n",
    "### Spectral Clustering\n",
    "\n",
    "Spectral clustering has the ability to fit non-convex data. However, unlike some other clustering methods, it cannot predict labels -- thus, a cross validation method can't be used.\n",
    "\n",
    "To get around this, we randomly subset our data multiple times and train a Spectral Clustering model on each subset with varying `n_neighbors`. Since Spectral Clustering makes heavy use of adjacency and Laplacian matrices, the time and space complexity is high. In fact, Spectral Clustering requires $O(n^2)$ memory and $O(n^3)$ time. A few thousand samples is relatively representative of our data, and we won't run into computing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcc3043-3334-4da6-8676-0cebe8e47e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_list = [1,5,10,20]\n",
    "spectral_ari = pd.DataFrame(index =  range(4), columns = [1,5,10,20])\n",
    "for i in range(4):\n",
    "    X_train, _, y_train, _ = train_test_split(fitted_pca, labels[\"shape_label\"], train_size=2000, random_state = i)\n",
    "    for nn in nn_list:\n",
    "        spectral = SpectralClustering(n_clusters = 4, n_neighbors = nn)\n",
    "        y_labels = spectral.fit(X_train).labels_\n",
    "        spectral_ari.loc[i, nn] = metrics.adjusted_rand_score(y_labels, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af8b3d5-315f-456a-8492-24b8656aac90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>5</th>\n",
       "      <th>10</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "      <td>-0.000056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.001063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001651</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>-0.001651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         5         10        20\n",
       "0 -0.000056 -0.000056 -0.000056 -0.000056\n",
       "1 -0.001063 -0.001063 -0.001063 -0.001063\n",
       "2 -0.000066 -0.000066 -0.000066 -0.000066\n",
       "3 -0.001651 -0.001651 -0.001651 -0.001651"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectral_ari"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71156961-302e-49b6-93c4-7078ac2e952f",
   "metadata": {},
   "source": [
    "Similar to our GMMs, spectral clustering fails to produce an acceptable result. With ARI's close to 0, we can conclude with our implementation of dimensionality reduction and clustering, the data is difficult to cluster cleanly. There can be various reasons for this. One may be that the data is not easily separable, a problem which compounds with dimensionality reduction. Another possible explanation is that the data structure does not fit the assumptions of GMM or spectral clustering; i.e, not Gaussian distributed or a lack of local relationship, respectively.\n",
    "\n",
    "Other clustering and dimensionality reduction methods are explored in other notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
